//
// The tokenizer of the lexer. Processes the raw character stream into tokens
// for the parser to use. Also does string internment, and creates the symbol
// tables (with scopes directed by the parser).
//
// STOLEN from Jackal compiler and hacked to bits for the build tool.
//

#INCLUDE "Bt.hjk"

PUBLIC LexCharTreatment : LexCharBehavior[256] = {
    [0] = CHAR_EOF,

    [' ']  = CHAR_WHITESPACE,
    ['\n'] = CHAR_WHITESPACE,
    ['\t'] = CHAR_WHITESPACE,

    ['^'] = CHAR_SPLIT,
    ['('] = CHAR_SPLIT,
    [')'] = CHAR_SPLIT,
    ['~'] = CHAR_SPLIT,
    [','] = CHAR_SPLIT,
    ['['] = CHAR_SPLIT,
    [']'] = CHAR_SPLIT,
    [':'] = CHAR_SPLIT,
    ['{'] = CHAR_SPLIT,
    ['}'] = CHAR_SPLIT,
    ['#'] = CHAR_SPLIT,

    ['='] = CHAR_COALESCE,
    ['&'] = CHAR_COALESCE,
    ['|'] = CHAR_COALESCE,
    ['!'] = CHAR_COALESCE,
    ['<'] = CHAR_COALESCE,
    ['>'] = CHAR_COALESCE,
    ['+'] = CHAR_COALESCE,
    ['-'] = CHAR_COALESCE,
    ['*'] = CHAR_COALESCE,
    ['/'] = CHAR_COALESCE,
    ['%'] = CHAR_COALESCE,
    ['.'] = CHAR_COALESCE,
    ['@'] = CHAR_COALESCE,
    ['$'] = CHAR_COALESCE,

    // Since CHAR_NORMAL is the first field in the enum, it is guaranteed to
    // be zero in value, so we don't have to explicitly set all the other
    // character behaviors to it, since any non-defined fields here will also
    // be zero.
}

#DEFINE LEX_TOKEN_STRING_SIZE 128

PUBLIC LexCurrentStream : ^LexStream = NULLPTR
PUBLIC LexFalseCount : ULONG = 0

LexPutbackStack : LexToken[LEX_PUTBACK_STACK_DEPTH]
LexPutbackStackPtr : ULONG = 0

STRUCT LexKeyword
    Entry : TlHashTableEntry, // MUST be at the beginning
    Type : LexTokenType,
    Subtype : LexTokenSubtype,
    TypeContext : UBYTE,
END

LexKeywordHashTable : TlHashTable

LexKeywordBumpArray : LexKeyword[128]
LexKeywordBumpIndex := 0

LexInternedStringZone : TlZone

FN LexInsertKeyword (
    IN name : ^UBYTE,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
    IN context : UBYTE,
)

    // Insert the keyword into the keyword hash table.
    // This is done every time the compiler is invoked so it should be VERY
    // fast. To this end we completely avoid dynamic allocation with this silly
    // keyword structure bump allocator. The hash table package does no dynamic
    // allocation, nor does it do any string copies (though it does iterate the
    // string to calculate the hash), so this should all be quite speedy.

    keyword := &LexKeywordBumpArray[LexKeywordBumpIndex]
    LexKeywordBumpIndex += 1

    keyword^.Type = type
    keyword^.Subtype = subtype
    keyword^.TypeContext = context

    TlInsertHashTable (
        &LexKeywordHashTable, // hashtable
        &keyword^.Entry, // entry
        name, // key
    )
END

FN LexError (
    IN token : ^LexToken,
    IN fmt : ^UBYTE,
    ... argv argc
)

    IF token THEN
        TlPrintByHandle (
            TlStdErr, // handle
            "%s:%d:%d: ", // fmt
            &token^.FileBlock^.IncludeName[0],
            token^.LineNumber,
            token^.LinePosition,
        )

    ELSE
        stream := LexCurrentStream

        TlPrintByHandle (
            TlStdErr, // handle
            "%s:%d:%d: ", // fmt
            &stream^.FileBlock^.IncludeName[0],
            stream^.LineNumber,
            stream^.LinePosition,
        )
    END

    TlPrintByVarTable (
        TlStdErr, // handle
        fmt, // fmt
        argv, // argv
        argc, // argc
    )

    TlErrorExit ()
END

FN LexInitialize ()

    dumpster : UBYTE

    // Create the file block for the source file.

    fileblock := FeCreateFileBlock (
        &FeInputFile[0], // includename
        OUT dumpster, // created
    )

    FeCopyPathFileBlock (
        fileblock, // fileblock
        &FeInputFile[0], // filepath
    )

    // Initialize the stream zone.

    LexInitializeStreamZone ()

    // Create the root file stream.

    filestream := LexCreateFileStream (
        fileblock, // fileblock
        FeInputFileHandle, // handle
    )

    LexPushStream ( filestream )

    // Initialize the interned string zone.

    TlInitializeZone (
        &LexInternedStringZone, // zone
        SIZEOF LexInternedString, // blocksize
    )

    // Initialize keyword hash table.

    TlInitializeHashTable ( &LexKeywordHashTable )

    LexInsertKeyword ( "END", TOKEN_END, 0, 0 )
    LexInsertKeyword ( "TARGET", TOKEN_TARGET, 0, 0 )

    LexInsertKeyword ( "FILE", TOKEN_TARGET_INFO, TOKEN_FILE, 0 )
    LexInsertKeyword ( "DEPEND", TOKEN_TARGET_INFO, TOKEN_DEPEND, 0 )
    LexInsertKeyword ( "SOURCES", TOKEN_TARGET_INFO, TOKEN_SOURCES, 0 )
    LexInsertKeyword ( "INCDIR", TOKEN_TARGET_INFO, TOKEN_JKLINCLUDE, 0 )
    LexInsertKeyword ( "LIBDIR", TOKEN_TARGET_INFO, TOKEN_LIBDIR, 0 )
    LexInsertKeyword ( "LINKOPT", TOKEN_TARGET_INFO, TOKEN_LINKOPT, 0 )
    LexInsertKeyword ( "JKLOPT", TOKEN_TARGET_INFO, TOKEN_JKLOPT, 0 )
    LexInsertKeyword ( "LINK", TOKEN_TARGET_INFO, TOKEN_LINK, 0 )
    LexInsertKeyword ( "DYLINK", TOKEN_TARGET_INFO, TOKEN_DYLINK, 0 )
    LexInsertKeyword ( "PREBUILT", TOKEN_TARGET_INFO, TOKEN_PREBUILT, 0 )
    LexInsertKeyword ( "OBJECTS", TOKEN_TARGET_INFO, TOKEN_OBJECTS, 0 )
END

FN (LexGetCharacterF) LexGetCharacter () : UBYTE

    // Return the next character in the source stream. Omits comments and text
    // that is killed by the "preprocessor", and notices preprocessor
    // directives, which it ships off to a special little interpreter.

    byte : UBYTE

    comment := FALSE

    WHILE TRUE DO
        byte = LexStreamNextCharacter ()

        IF NOT byte THEN
            // EOF has been reached.

            BREAK
        END

        // We care about backslashes and about knowing whether we're in a
        // string or not in this routine, because otherwise we will
        // mis-identify things as comments or directives that aren't.

        IF comment THEN
            IF byte != '\n' THEN
                CONTINUE
            END

            comment = FALSE

            // Make sure this newline is seen by the higher level code,
            // otherwise a token that is immediately followed by a comment
            // with no interfering whitespace may not terminate properly.

        ELSEIF LexCurrentStream^.Backslash THEN
            LexCurrentStream^.Backslash = FALSE

        ELSEIF byte == '\\' THEN
            LexCurrentStream^.Backslash = TRUE

        ELSEIF LexCurrentStream^.InString THEN
            LexCurrentStream^.InString = NOT (byte == '\"')

        ELSEIF LexCurrentStream^.InLiteral THEN
            LexCurrentStream^.InLiteral = NOT (byte == 0x27) // single-quote

        ELSEIF byte == '/' THEN
            // We have to figure out if this is a comment or not. This
            // requires looking at the next character and seeing if it is also
            // a forward slash character. If it is, we set comment to true. If
            // it isn't, we pass this forward slash along and set putback to
            // the value of the character we peeked, causing it to be returned
            // on the next call.

            nextbyte := LexStreamNextCharacter ()

            IF nextbyte == '/' THEN
                comment = TRUE

                CONTINUE
            END

            LexCurrentStream^.Putback = nextbyte

        ELSEIF byte == '\"' THEN
            LexCurrentStream^.InString = TRUE

        ELSEIF byte == 0x27 THEN // single-quote
            LexCurrentStream^.InLiteral = TRUE

        ELSEIF byte == '#' THEN
            // We found a preprocessor directive! We dispatch to
            // the preprocessor regardless of the value of the false count.
            // This gives it an opportunity to manipulate the false count, but
            // it must be sure to ignore directives that aren't relevant.

            LexParseDirective ()

            // The preprocessor consumed the newline character terminating the
            // directive, so we return one here for good measure.

            byte = '\n'
        END

        IF LexFalseCount THEN
            // A non-zero false count causes all characters to be ignored
            // until this situation is resolved by a preprocessor directive.
            // This is used for conditional compilation.

            CONTINUE
        END

        BREAK
    END

    RETURN byte
END

#DEFINE TOKEN_FLAG_SPLIT    1
#DEFINE TOKEN_FLAG_COALESCE 2
#DEFINE TOKEN_FLAG_STRING   4
#DEFINE TOKEN_FLAG_CHARLIT  8
#DEFINE TOKEN_FLAG_UPPER    16
#DEFINE TOKEN_FLAG_FOGETTABOUTIT 32

FN LexGetStringTokenInternal (
    IN token : ^LexToken,
    IN terminator : UBYTE,
    IN dynamicbuffer : ^TlDynamicBuffer,
    IN buffer : ^UBYTE,
    OUT length : ULONG,
)

    len := 0
    isbackslash := FALSE

    WHILE TRUE DO
        byte := LexGetCharacter ()

        IF NOT byte THEN
            LexError ( token, "String token terminated by EOF!\n" )
        END

        IF NOT isbackslash THEN
            IF byte == terminator THEN
                // String is done.

                BREAK

            ELSEIF byte == '\\' THEN
                isbackslash = TRUE

                CONTINUE
            END
        ELSE
            isbackslash = FALSE

            IF byte == 'n' THEN
                byte = '\n'
            
            ELSEIF byte == 't' THEN
                byte = '\t'
            
            ELSEIF byte == 'r' THEN
                byte = '\r'
            
            ELSEIF byte == 'b' THEN
                byte = '\b'
            
            ELSEIF byte == '[' THEN
                byte = '\['
            END
        END

        IF dynamicbuffer THEN
            TlInsertDynamicBuffer ( dynamicbuffer, byte )
        
        ELSEIF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexError ( token, "Token length exceeds maximum.\n" )
        
        ELSE
            buffer[len] = byte
            len += 1
        END
    END

    IF dynamicbuffer THEN
        TlInsertDynamicBuffer ( dynamicbuffer, 0 )
    ELSE
        buffer[len] = 0
        length = len
    END
END

FN LexCrunchCharacterLiteral (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : UWORD

    num : UWORD = 0
    i := 0

    WHILE buffer[i] DO
        lastnum := num

        num *= 256

        IF lastnum != 0 AND num / lastnum != 256 THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        num += buffer[i]

        IF num < lastnum THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        i += 1
    END

    RETURN num
END

FN LexCrunchNumber (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : UWORD

    num : UWORD = 0
    i := 0

    IF buffer[0] == '0' THEN
        // This number is either a literal zero, octal, or hexadecimal.

        IF NOT buffer[1] THEN
            // Literal zero.

            RETURN 0
        END

        IF buffer[1] == 'x' THEN
            // Hexadecimal.

            IF NOT buffer[2] THEN
                LexError ( token, "Unfinished hex number.\n" )
            END

            i = 2

            WHILE buffer[i] DO
                lastnum := num

                num *= 16

                // Check for overflow.

                IF lastnum != 0 AND num / lastnum != 16 THEN
                    LexError ( token, "Number does not fit in machine word.\n" )
                END

                IF buffer[i] >= '0' AND buffer[i] <= '9' THEN
                    num += buffer[i] - '0'
                
                ELSEIF buffer[i] >= 'A' AND buffer[i] <= 'F' THEN
                    num += buffer[i] - 'A' + 10
                
                ELSEIF buffer[i] >= 'a' AND buffer[i] <= 'f' THEN
                    num += buffer[i] - 'a' + 10

                ELSE
                    LexError ( token, "Malformed hex number.\n" )
                END

                // Check for overflow.

                IF num < lastnum THEN
                    LexError ( token, "Number does not fit in machine word.\n" )
                END

                i += 1
            END

            RETURN num
        END

        // Octal.

        i = 1

        WHILE buffer[i] DO
            IF NOT (buffer[i] >= '0' AND buffer[i] <= '7') THEN
                LexError ( token, "Malformed octal number.\n" )
            END

            lastnum := num

            num *= 8

            // Check for overflow.

            IF lastnum != 0 AND num / lastnum != 8 THEN
                LexError ( token, "Number does not fit in machine word.\n" )
            END

            num += buffer[i] - '0'

            // Check for overflow.

            IF num < lastnum THEN
                LexError ( token, "Number does not fit in machine word.\n" )
            END

            i += 1
        END

        RETURN num
    END

    // Decimal.

    WHILE buffer[i] DO
        IF NOT (buffer[i] >= '0' AND buffer[i] <= '9') THEN
            LexError ( token, "Malformed decimal number.\n" )
        END

        lastnum := num

        num *= 10

        // Check for overflow.

        IF lastnum != 0 AND num / lastnum != 10 THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        num += buffer[i] - '0'

        // Check for overflow.

        IF num < lastnum THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        i += 1
    END

    RETURN num
END

FN LexGetTokenContents (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
    OUT length : ULONG,
    OUT internedstring : ^LexInternedString,
) : UBYTE

    // Reads the string of the next token into the provided buffer. Identifies
    // macros and expands them. Returns a set of flags indicating things about
    // the token; for instance, if EOF has been reached (i.e. there is no
    // token), if the token is all uppercase, and if the token is a SPLIT or
    // COALESCE token. The buffer is assumed to be LEX_TOKEN_STRING_SIZE bytes
    // in length or greater.

    // We GOTO LoopOnMacro in the case that the token turns out to be a macro
    // that needs expansion (this is intentionally picked over the alternative
    // of wrapping this routine in a WHILE TRUE loop).

    @LoopOnMacro

    length = 0

    // Skip leading whitespace.

    stream := LexCurrentStream

    IF stream THEN
        token^.FileBlock = stream^.FileBlock
        token^.LineNumber = stream^.LineNumber
        token^.LinePosition = stream^.LinePosition
    END

    byte := LexGetCharacter ()

    WHILE LexCharTreatment[byte] == CHAR_WHITESPACE DO
        // Re-capture the stream location information for each character of
        // whitespace consumed, until we get the real thing.

        stream = LexCurrentStream

        IF stream THEN
            token^.FileBlock = stream^.FileBlock
            token^.LineNumber = stream^.LineNumber
            token^.LinePosition = stream^.LinePosition
        END

        byte = LexGetCharacter ()
    END

    IF NOT byte THEN
        // EOF encountered.

        RETURN 0
    END

    IF byte == '\"' THEN
        // This is a string token. Allocate a LexInternedString.

        internedstring = CAST TlAllocateFromZone (
            &LexInternedStringZone, // zone
        ) TO ^LexInternedString

        dynamicbuffer := &internedstring^.DynamicBuffer

        TlInitializeDynamicBuffer ( dynamicbuffer )

        dumpster : ULONG

        LexGetStringTokenInternal (
            token, // token
            '\"', // terminator
            dynamicbuffer, // dynamicbuffer
            NULLPTR, // buffer
            OUT dumpster, // length
        )

        RETURN TOKEN_FLAG_STRING
    END

    IF byte == 0x27 THEN // single-quote
        // This is a character literal.

        LexGetStringTokenInternal (
            token, // token
            0x27, // terminator
            NULLPTR, // dynamicbuffer
            buffer, // buffer
            OUT length, // length
        )

        RETURN TOKEN_FLAG_CHARLIT
    END

    IF LexCharTreatment[byte] == CHAR_SPLIT THEN
        // This is a split token. It consists of exactly this character and no
        // more.

        length = 1
        buffer[0] = byte
        buffer[1] = 0

        RETURN TOKEN_FLAG_SPLIT
    END

    IF LexCharTreatment[byte] == CHAR_COALESCE THEN
        // This is a coalesce token. It consists of this character and the
        // following coalesce characters, and no more.

        len := 0

        WHILE LexCharTreatment[byte] == CHAR_COALESCE DO
            IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
                LexError ( token, "Token length exceeds maximum.\n" )
            END

            buffer[len] = byte
            len += 1

            byte = LexGetCharacter ()
        END

        buffer[len] = 0
        length = len

        IF LexCharTreatment[byte] != CHAR_WHITESPACE THEN
            // Set the putback character to the last one we saw, since that is part
            // of the next token.

            LexCurrentStream^.CoalescePutback = byte
        END

        RETURN TOKEN_FLAG_COALESCE
    END

    // This is a normal token. All we have to track is whether it consists only
    // of uppercase letters A-Z, in which case it must be a keyword. It could
    // also turn out to be a macro, in which case we must add it to the stream
    // stack and loop here.

    uppercaseonly := byte >= 'A' AND byte <= 'Z'

    len := 0

    WHILE LexCharTreatment[byte] == CHAR_NORMAL DO
        IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexError ( token, "Token length exceeds maximum.\n" )
        END

        IF uppercaseonly THEN
            uppercaseonly = byte >= 'A' AND byte <= 'Z'
        END

        buffer[len] = byte
        len += 1

        byte = LexGetCharacter ()
    END

    buffer[len] = 0
    length = len

    IF LexCharTreatment[byte] == CHAR_SPLIT OR
        LexCharTreatment[byte] == CHAR_COALESCE THEN

        LexCurrentStream^.CoalescePutback = byte
    END

    IF uppercaseonly AND len > 1 THEN
        RETURN TOKEN_FLAG_UPPER
    END

    // Check if this is a defined macro name. If so, place that macro on the top
    // of the stream stack and loop to the beginning of this procedure.
    //
    // N.B. Jackal macros are "hygienic", meaning that they have no access to
    // the "parent scope", except via their arguments, which become special
    // macros that restore the parent scope during their expansion. This
    // requires associating scopes with streams, so that we know what to restore
    // when we pop a stream off the stream stack.

    macro := CAST TlLookupSymbolTable (
        LexCurrentMacroScope, // symboltable
        &buffer[0], // name
    ) TO ^LexMacro

    IF NOT macro THEN
        // Not a defined macro, just return the token.

        RETURN 0
    END

    // It is a macro! Expand it and loop, so our caller gets the next "real"
    // token.

    LexExpandMacro (
        macro, // macro
        token, // token
    )

    GOTO LoopOnMacro
END

FN LexNextToken (
    IN token : ^LexToken,
)

    // Upper level part of token getting.
    // Receives a token string and some flags, and does some extra crunching on
    // it. Ignores the putback stack.

    internedstring : ^LexInternedString

    buffer : UBYTE[LEX_TOKEN_STRING_SIZE]
    length : ULONG

    tokenflag := LexGetTokenContents (
        token, // token
        &buffer[0], // buffer
        OUT length, // length
        OUT internedstring, // internedstring
    )

    IF tokenflag & ( TOKEN_FLAG_SPLIT | TOKEN_FLAG_COALESCE |
       TOKEN_FLAG_UPPER ) THEN

        // This must be a keyword token. If it doesn't hash to a keyword,
        // there's a syntax error.

        keyword := CAST TlLookupHashTable (
            &LexKeywordHashTable, // hashtable
            &buffer[0], // key
        ) TO ^LexKeyword

        IF NOT keyword THEN
            IF tokenflag & TOKEN_FLAG_UPPER THEN
                LexError ( token,
                "Bad use of uppercase alphabetic token (must be keyword).\n" )
            END

            LexError ( token, "Unknown keyword\n" )
        END

        token^.Type = keyword^.Type
        token^.Subtype = keyword^.Subtype
        token^.TypeContext = keyword^.TypeContext

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_CHARLIT THEN
        // This is a character literal token. We turn those into numerical
        // tokens.

        token^.Type = TOKEN_NUMBER
        token^.Payload = CAST LexCrunchCharacterLiteral (
            token, // token
            &buffer[0], // buffer
        ) TO ^VOID

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_STRING THEN
        // This is a string token. Record the interned string.

        token^.Type = TOKEN_STRING
        token^.Payload = internedstring

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_FOGETTABOUTIT THEN
        // This is an ignored macro.

        token^.Type = TOKEN_EOF

        LEAVE
    END

    IF NOT length THEN
        // EOF has been reached.

        token^.Type = TOKEN_EOF

        LEAVE
    END

    IF buffer[0] >= '0' AND buffer[0] <= '9' THEN
        // This is a numerical token.

        token^.Type = TOKEN_NUMBER
        token^.Payload = CAST LexCrunchNumber (
            token, // token
            &buffer[0], // buffer
        ) TO ^VOID

        LEAVE
    END

    LexError ( token, "Unknown token\n" )
END

FN LexCopyToken (
    IN dest : ^LexToken,
    IN src : ^LexToken,
)

    // Copy all the fields of a source token into a destination token.

    dest^.Payload = src^.Payload
    dest^.FileBlock = src^.FileBlock
    dest^.LineNumber = src^.LineNumber
    dest^.LinePosition = src^.LinePosition
    dest^.Type = src^.Type
    dest^.Subtype = src^.Subtype
    dest^.TypeContext = src^.TypeContext
END

FN LexGetToken (
    IN token : ^LexToken,
)

    // Read a token from the putback stack if one is present, otherwise read
    // from the source stream.

    index := LexPutbackStackPtr

    IF index THEN
        LexCopyToken (
            token, // dest
            &LexPutbackStack[index - 1], // src
        )

        LexPutbackStackPtr = index - 1

        LEAVE
    END

    LexNextToken ( token )
END

FN LexPutbackToken (
    IN token : ^LexToken,
)

    // Put the token on top of the putback stack.

    index := LexPutbackStackPtr

    IF index == LEX_PUTBACK_STACK_DEPTH THEN
        TlInternalError ( "Lexer putback stack overflow", 0, 0, 0 )
    END

    LexCopyToken (
        &LexPutbackStack[index], // dest
        token, // src
    )

    LexPutbackStackPtr = index + 1
END

FN LexMatchToken (
    IN token : ^LexToken,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
) : UBYTE

    // Read the next token into the caller's token structure, but only if it
    // matches the type and subtype. If the type or subtype are specified as
    // TOKEN_ANY or TOKEN_SUBTYPE_ANY respectively, then it will match any type
    // or subtype of token. Returns TRUE if a token was read, FALSE otherwise.

    index := LexPutbackStackPtr

    IF NOT index THEN
        // The putback stack is empty, read one token directly into the top.

        LexNextToken (
            &LexPutbackStack[0], // token
        )

        LexPutbackStackPtr = 1
        index = 1
    END

    matchtoken := &LexPutbackStack[index - 1]

    IF token THEN
        LexCopyToken (
            token, // dest
            matchtoken, // src
        )
    END

    IF matchtoken^.Type != type THEN
        // Not a match.

        IF matchtoken^.Type == TOKEN_EOF THEN
            // Don't save EOF tokens.

            LexPutbackStackPtr -= 1
        END

        RETURN FALSE
    END

    IF NOT subtype OR matchtoken^.Subtype == subtype THEN
        // Token matches! Pop it.

        LexPutbackStackPtr = index - 1

        RETURN TRUE
    END

    RETURN FALSE
END
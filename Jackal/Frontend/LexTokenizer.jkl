//
// The tokenizer of the lexer. Processes the raw character stream into tokens
// for the parser to use. Also does string internment, and creates the symbol
// tables (with scopes directed by the parser).
//

#INCLUDE "<inc>/Lexer.hjk"

PUBLIC LexCharTreatment : LexCharBehavior[256] = {
    [0] = CHAR_EOF,

    [' ']  = CHAR_WHITESPACE,
    ['\n'] = CHAR_WHITESPACE,
    ['\t'] = CHAR_WHITESPACE,

    ['^'] = CHAR_SPLIT,
    ['('] = CHAR_SPLIT,
    [')'] = CHAR_SPLIT,
    ['~'] = CHAR_SPLIT,
    [','] = CHAR_SPLIT,
    ['['] = CHAR_SPLIT,
    [']'] = CHAR_SPLIT,
    [':'] = CHAR_SPLIT,
    ['{'] = CHAR_SPLIT,
    ['}'] = CHAR_SPLIT,
    ['#'] = CHAR_SPLIT,

    ['='] = CHAR_COALESCE,
    ['&'] = CHAR_COALESCE,
    ['|'] = CHAR_COALESCE,
    ['!'] = CHAR_COALESCE,
    ['<'] = CHAR_COALESCE,
    ['>'] = CHAR_COALESCE,
    ['+'] = CHAR_COALESCE,
    ['-'] = CHAR_COALESCE,
    ['*'] = CHAR_COALESCE,
    ['/'] = CHAR_COALESCE,
    ['%'] = CHAR_COALESCE,
    ['.'] = CHAR_COALESCE,
    ['@'] = CHAR_COALESCE,
    ['$'] = CHAR_COALESCE,

    // Since CHAR_NORMAL is the first field in the enum, it is guaranteed to
    // be zero in value, so we don't have to explicitly set all the other
    // character behaviors to it, since any non-defined fields here will also
    // be zero.
}

#DEFINE LEX_TOKEN_STRING_SIZE 128

PUBLIC LexCurrentStream : ^LexStream = NULLPTR
PUBLIC LexRootScope : ^TlSymbolTable = NULLPTR
PUBLIC LexCurrentScope : ^TlSymbolTable = NULLPTR
PUBLIC LexFalseCount : ULONG = 0

LexCurrentOverlayScope : ^TlSymbolTable = NULLPTR

LexPutbackStack : LexToken[LEX_PUTBACK_STACK_DEPTH]
LexPutbackStackPtr : ULONG = 0

LexInternedStringZone : TlZone

STRUCT LexKeyword
    Entry : TlHashTableEntry, // MUST be at the beginning
    Type : LexTokenType,
    Subtype : LexTokenSubtype,
    TypeContext : UBYTE,
END

LexKeywordHashTable : TlHashTable

FN LexInsertKeyword (
    IN name : ^UBYTE,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
    IN context : UBYTE,
)

    // Insert the keyword into the keyword hash table.
    // This is done every time the compiler is invoked so it should be VERY
    // fast. To this end we completely avoid dynamic allocation with this silly
    // keyword structure bump allocator. The hash table package does no dynamic
    // allocation, nor does it do any string copies (though it does iterate the
    // string to calculate the hash), so this should all be quite speedy.

    keyword : ^LexKeyword

    status := TlBumpAlloc (
        SIZEOF LexKeyword, // bytes
        OUT keyword, // ptr
    )

    IF status THEN
        TlInternalError ( "Failed to initialize keywords", 0, 0, 0 )
    END

    keyword^.Type = type
    keyword^.Subtype = subtype
    keyword^.TypeContext = context

    TlInsertHashTable (
        &LexKeywordHashTable, // hashtable
        &keyword^.Entry, // entry
        name, // key
    )
END

FN (TlHashTableEnumeratorF) LexDeleteSymbol (
    IN entry : ^TlHashTableEntry,
)

    // We can't free symbols because these structures are referenced throughout
    // the phases of the compiler long after the front-end finishes.

    // TlFreeToZone (
    //     &LexSymbolZone, // zone
    //     entry, // block
    // )
END

FN LexCreateSymbolTable (
    IN outerscope : ^TlSymbolTable,
) : ^TlSymbolTable

    RETURN TlCreateSymbolTable (
        outerscope, // outerscope
        &LexDeleteSymbol, // deletefunc
    )
END

FN LexError (
    IN token : ^LexToken,
    IN fmt : ^UBYTE,
    ... argv argc
)

    stream := LexCurrentStream

    dotrace := FALSE

    IF stream THEN
        dotrace = stream^.Previous != 0
    END

    IF token THEN
        IF token^.Macro THEN
            TlPrintByHandle (
                TlStdErr, // handle
                "%s:%d:%d: ", // fmt
                &token^.Macro^.Name[0],
                token^.LineNumber,
                token^.LinePosition,
            )

            dotrace = TRUE

        ELSE
            TlPrintByHandle (
                TlStdErr, // handle
                "%s:%d:%d: ", // fmt
                &token^.FileBlock^.IncludeName[0],
                token^.LineNumber,
                token^.LinePosition,
            )
        END

    ELSE
        TlPrintByHandle (
            TlStdErr, // handle
            "%s:%d:%d: ", // fmt
            &stream^.FileBlock^.IncludeName[0],
            stream^.LineNumber,
            stream^.LinePosition,
        )
    END

    TlPrintByVarTable (
        TlStdErr, // handle
        fmt, // fmt
        argv, // argv
        argc, // argc
    )

    IF dotrace THEN
        // Perform a "backtrace" of the streams to indicate how we got here.

        WHILE stream DO
            IF stream^.Macro THEN
                TlPrintByHandle (
                    TlStdErr, // handle
                    "    %s (macro) near line %d", // fmt
                    &stream^.Macro^.Name[0],
                    stream^.LineNumber,
                    &stream^.Macro^.FileBlock
                )

                IF stream^.Macro^.FileBlock THEN
                    TlPrintByHandle (
                        TlStdErr, // handle
                        " (defined in %s)", // fmt
                        &stream^.Macro^.FileBlock^.IncludeName[0],
                    )

                ELSEIF stream^.Macro^.ArgumentOf THEN
                    TlPrintByHandle (
                        TlStdErr, // handle
                        " (argument to macro %s)", // fmt
                        &stream^.Macro^.ArgumentOf^.Name[0],
                    )
                END

                TlPrintByHandle (
                    TlStdErr, // handle
                    ".\n", // fmt
                )

            ELSE
                TlPrintByHandle (
                    TlStdErr, // handle
                    "    %s near line %d.\n", // fmt
                    &stream^.FileBlock^.IncludeName[0],
                    stream^.LineNumber,
                )
            END

            stream = stream^.Previous
        END
    END

    TlErrorExit ()
END

FN LexInitialize ()

    dumpster : UBYTE

    // Create the file block for the source file.

    fileblock := FeCreateFileBlock (
        &FeInputFile[0], // includename
        OUT dumpster, // created
    )

    FeCopyPathFileBlock (
        fileblock, // fileblock
        &FeInputFile[0], // filepath
    )

    FeRootFileBlock = fileblock

    // Initialize the section package.

    LexInitializeSectionStuff ()

    // Initialize the stream zone.

    LexInitializeStreamZone ()

    // Create the root file stream.

    filestream := LexCreateFileStream (
        fileblock, // fileblock
        FeInputFileHandle, // handle
    )

    LexPushStream ( filestream )

    // Create the global symbol table scope.

    LexRootScope = LexCreateSymbolTable ( NULLPTR )

    LexCurrentScope = LexRootScope

    // Initialize the interned string zone.

    TlInitializeZone (
        &LexInternedStringZone, // zone
        SIZEOF LexInternedString, // blocksize
    )

    // Initialize keyword hash table.

    TlInitializeHashTable ( &LexKeywordHashTable )

    LexInsertKeyword ( "AND", TOKEN_OPER, TOKEN_AND, 0 )
    LexInsertKeyword ( "BREAK", TOKEN_STATEMENT, TOKEN_BREAK, 0 )
    LexInsertKeyword ( "BYTE", TOKEN_PTYPE, TOKEN_BYTE, PRIM_TYPE_BYTE )
    LexInsertKeyword ( "CAST", TOKEN_OPER, TOKEN_CAST, 0 )
    LexInsertKeyword ( "CONTAINEROF", TOKEN_OPER, TOKEN_CONTAINEROF, 0 )
    LexInsertKeyword ( "CONTINUE", TOKEN_STATEMENT, TOKEN_CONTINUE, 0 )
    LexInsertKeyword ( "DO", TOKEN_OTHER, TOKEN_DO, 0 )
    LexInsertKeyword ( "ELSE", TOKEN_TERMINATOR, TOKEN_ELSE, 0 )
    LexInsertKeyword ( "ELSEIF", TOKEN_TERMINATOR, TOKEN_ELSEIF, 0 )
    LexInsertKeyword ( "END", TOKEN_TERMINATOR, TOKEN_END, 0 )
    LexInsertKeyword ( "ENUM", TOKEN_DECL, TOKEN_ENUM, 0 )
    LexInsertKeyword ( "EXTERN", TOKEN_DECL, TOKEN_EXTERN, 0 )
    LexInsertKeyword ( "FALSE", TOKEN_VALUE, TOKEN_FALSE, 0 )
    LexInsertKeyword ( "FN", TOKEN_DECL, TOKEN_FN, 0 )
    LexInsertKeyword ( "FNPTR", TOKEN_DECL, TOKEN_FNPTR, 0 )
    LexInsertKeyword ( "GOTO", TOKEN_STATEMENT, TOKEN_GOTO, 0 )
    LexInsertKeyword ( "IF", TOKEN_STATEMENT, TOKEN_IF, 0 )
    LexInsertKeyword ( "IN", TOKEN_ARGSPEC, TOKEN_IN, 0 )
    LexInsertKeyword ( "INT", TOKEN_PTYPE, TOKEN_INT, PRIM_TYPE_INT )
    LexInsertKeyword ( "LEAVE", TOKEN_STATEMENT, TOKEN_LEAVE, 0 )
    LexInsertKeyword ( "LONG", TOKEN_PTYPE, TOKEN_LONG, PRIM_TYPE_LONG )
    LexInsertKeyword ( "NOT", TOKEN_OPER, TOKEN_NOT, 0 )
    LexInsertKeyword ( "NULLPTR", TOKEN_VALUE, TOKEN_NULLPTR, 0 )
    LexInsertKeyword ( "OR", TOKEN_OPER, TOKEN_OR, 0 )
    LexInsertKeyword ( "OUT", TOKEN_ARGSPEC, TOKEN_OUT, 0 )
    LexInsertKeyword ( "PACKED", TOKEN_OTHER, TOKEN_PACKED, 0 )
    LexInsertKeyword ( "PUBLIC", TOKEN_DECL, TOKEN_PUBLIC, 0 )
    LexInsertKeyword ( "RETURN", TOKEN_STATEMENT, TOKEN_RETURN, 0 )
    LexInsertKeyword ( "SIZEOF", TOKEN_SIZEOF, 0, 0 )
    LexInsertKeyword ( "OFFSETOF", TOKEN_OFFSETOF, 0, 0 )
    LexInsertKeyword ( "SIZEOFVALUE", TOKEN_OPER, TOKEN_SIZEOFVALUE, 0 )
    LexInsertKeyword ( "STRUCT", TOKEN_DECL, TOKEN_STRUCT, 0 )
    LexInsertKeyword ( "THEN", TOKEN_OTHER, TOKEN_THEN, 0 )
    LexInsertKeyword ( "TO", TOKEN_OTHER, TOKEN_TO, 0 )
    LexInsertKeyword ( "TRUE", TOKEN_VALUE, TOKEN_TRUE, 0 )
    LexInsertKeyword ( "TYPE", TOKEN_DECL, TOKEN_TYPE, 0 )
    LexInsertKeyword ( "UBYTE", TOKEN_PTYPE, TOKEN_UBYTE, PRIM_TYPE_UBYTE )
    LexInsertKeyword ( "UINT", TOKEN_PTYPE, TOKEN_UINT, PRIM_TYPE_UINT )
    LexInsertKeyword ( "ULONG", TOKEN_PTYPE, TOKEN_ULONG, PRIM_TYPE_ULONG )
    LexInsertKeyword ( "UNION", TOKEN_DECL, TOKEN_UNION, 0 )
    LexInsertKeyword ( "VOID", TOKEN_PTYPE, TOKEN_VOID, PRIM_TYPE_VOID )
    LexInsertKeyword ( "WHILE", TOKEN_STATEMENT, TOKEN_WHILE, 0 )
    LexInsertKeyword ( "BARRIER", TOKEN_STATEMENT, TOKEN_BARRIER, 0 )
    LexInsertKeyword ( "INSERTASM", TOKEN_STATEMENT, TOKEN_INSERTASM, 0 )
    LexInsertKeyword ( "NOTHING", TOKEN_STATEMENT, TOKEN_NOTHING, 0 )
    LexInsertKeyword ( "EXPORT", TOKEN_DECL, TOKEN_EXPORT, 0 )
    LexInsertKeyword ( "PRIVATE", TOKEN_DECL, TOKEN_PRIVATE, 0 )
    LexInsertKeyword ( "UQUAD", TOKEN_PTYPE, TOKEN_UQUAD, PRIM_TYPE_UQUAD )
    LexInsertKeyword ( "QUAD", TOKEN_PTYPE, TOKEN_QUAD, PRIM_TYPE_QUAD )
    LexInsertKeyword ( "ROR", TOKEN_OPER, TOKEN_ROR, 0 )

    LexInsertKeyword ( "UWORD", TOKEN_PTYPE, TOKEN_UWORD,
        JklTargetInfo^.LargestPrimitive )

    LexInsertKeyword ( "WORD", TOKEN_PTYPE, TOKEN_WORD,
        JklTargetInfo^.LargestSignedPrimitive )

    LexInsertKeyword ( ")", TOKEN_RPAREN, 0, 0 )
    LexInsertKeyword ( "]", TOKEN_RBRACKET, 0, 0 )
    LexInsertKeyword ( ":", TOKEN_COLON, 0, 0 )
    LexInsertKeyword ( "{", TOKEN_LBRACE, 0, 0 )
    LexInsertKeyword ( "}", TOKEN_RBRACE, 0, 0 )
    LexInsertKeyword ( ",", TOKEN_COMMA, 0, 0 )

    LexInsertKeyword ( "==", TOKEN_OPER, TOKEN_EQUIV, 0 )
    LexInsertKeyword ( "!=", TOKEN_OPER, TOKEN_NOTEQUIV, 0 )
    LexInsertKeyword ( "&", TOKEN_OPER, TOKEN_BITAND, 0 )
    LexInsertKeyword ( "|", TOKEN_OPER, TOKEN_BITOR, 0 )
    LexInsertKeyword ( "<", TOKEN_OPER, TOKEN_LESSTHAN, 0 )
    LexInsertKeyword ( ">", TOKEN_OPER, TOKEN_GREATERTHAN, 0 )
    LexInsertKeyword ( "<=", TOKEN_OPER, TOKEN_LTEQ, 0 )
    LexInsertKeyword ( ">=", TOKEN_OPER, TOKEN_GTEQ, 0 )
    LexInsertKeyword ( "+", TOKEN_OPER, TOKEN_PLUS, 0 )
    LexInsertKeyword ( "-", TOKEN_OPER, TOKEN_MINUS, 0 )
    LexInsertKeyword ( "/", TOKEN_OPER, TOKEN_DIVIDE, 0 )
    LexInsertKeyword ( "%", TOKEN_OPER, TOKEN_MODULO, 0 )
    LexInsertKeyword ( ".", TOKEN_OPER, TOKEN_DOT, 0 )
    LexInsertKeyword ( "@", TOKEN_STATEMENT, TOKEN_LABEL, 0 )
    LexInsertKeyword ( "$", TOKEN_OPER, TOKEN_BITXOR, 0 )
    LexInsertKeyword ( "<<", TOKEN_OPER, TOKEN_LEFTSHIFT, 0 )
    LexInsertKeyword ( ">>", TOKEN_OPER, TOKEN_RIGHTSHIFT, 0 )
    LexInsertKeyword ( "~", TOKEN_OPER, TOKEN_BITNOT, 0 )
    LexInsertKeyword ( "*", TOKEN_OPER, TOKEN_MUL, 0 )
    LexInsertKeyword ( "...", TOKEN_ARGSPEC, TOKEN_VARARG, 0 )

    LexInsertKeyword ( "[", TOKEN_OPER, TOKEN_LBRACKET, 0 )
    LexInsertKeyword ( "^", TOKEN_OPER, TOKEN_CARET, 0 )
    LexInsertKeyword ( "(", TOKEN_OPER, TOKEN_LPAREN, 0 )

    LexInsertKeyword ( "=", TOKEN_ASSIGN, TOKEN_EQUALS, 0 )
    LexInsertKeyword ( "+=", TOKEN_ASSIGN, TOKEN_PLUSEQUALS, 0 )
    LexInsertKeyword ( "-=", TOKEN_ASSIGN, TOKEN_MINUSEQUALS, 0 )
    LexInsertKeyword ( "*=", TOKEN_ASSIGN, TOKEN_MULEQUALS, 0 )
    LexInsertKeyword ( "/=", TOKEN_ASSIGN, TOKEN_DIVEQUALS, 0 )
    LexInsertKeyword ( "%=", TOKEN_ASSIGN, TOKEN_MODEQUALS, 0 )
    LexInsertKeyword ( "&=", TOKEN_ASSIGN, TOKEN_ANDEQUALS, 0 )
    LexInsertKeyword ( "|=", TOKEN_ASSIGN, TOKEN_OREQUALS, 0 )
    LexInsertKeyword ( "$=", TOKEN_ASSIGN, TOKEN_XOREQUALS, 0 )
    LexInsertKeyword ( "<<=", TOKEN_ASSIGN, TOKEN_LSHEQUALS, 0 )
    LexInsertKeyword ( ">>=", TOKEN_ASSIGN, TOKEN_RSHEQUALS, 0 )
END

FN (LexGetCharacterF) LexGetCharacter () : UBYTE

    // Return the next character in the source stream. Omits comments and text
    // that is killed by the "preprocessor", and notices preprocessor
    // directives, which it ships off to a special little interpreter.

    byte : UBYTE

    comment := FALSE

    WHILE TRUE DO
        byte = LexStreamNextCharacter ()

        IF NOT byte THEN
            // EOF has been reached.

            BREAK
        END

        // We care about backslashes and about knowing whether we're in a
        // string or not in this routine, because otherwise we will
        // mis-identify things as comments or directives that aren't.

        IF comment THEN
            IF byte != '\n' THEN
                CONTINUE
            END

            comment = FALSE

            // Make sure this newline is seen by the higher level code,
            // otherwise a token that is immediately followed by a comment
            // with no interfering whitespace may not terminate properly.

        ELSEIF LexCurrentStream^.Backslash THEN
            LexCurrentStream^.Backslash = FALSE

        ELSEIF byte == '\\' THEN
            LexCurrentStream^.Backslash = TRUE

        ELSEIF LexCurrentStream^.InString THEN
            LexCurrentStream^.InString = NOT (byte == '\"')

        ELSEIF LexCurrentStream^.InLiteral THEN
            LexCurrentStream^.InLiteral = NOT (byte == 0x27) // single-quote

        ELSEIF byte == '/' THEN
            // We have to figure out if this is a comment or not. This
            // requires looking at the next character and seeing if it is also
            // a forward slash character. If it is, we set comment to true. If
            // it isn't, we pass this forward slash along and set putback to
            // the value of the character we peeked, causing it to be returned
            // on the next call.

            nextbyte := LexStreamNextCharacter ()

            IF nextbyte == '/' THEN
                comment = TRUE

                CONTINUE
            END

            LexCurrentStream^.Putback = nextbyte

        ELSEIF byte == '\"' THEN
            LexCurrentStream^.InString = TRUE

        ELSEIF byte == 0x27 THEN // single-quote
            LexCurrentStream^.InLiteral = TRUE

        ELSEIF byte == '#' THEN
            // We found a preprocessor directive! We dispatch to
            // the preprocessor regardless of the value of the false count.
            // This gives it an opportunity to manipulate the false count, but
            // it must be sure to ignore directives that aren't relevant.

            LexParseDirective ()

            // The preprocessor consumed the newline character terminating the
            // directive, so we return one here for good measure.

            byte = '\n'
        END

        IF LexFalseCount THEN
            // A non-zero false count causes all characters to be ignored
            // until this situation is resolved by a preprocessor directive.
            // This is used for conditional compilation.

            CONTINUE
        END

        BREAK
    END

    RETURN byte
END

#DEFINE TOKEN_FLAG_SPLIT    1
#DEFINE TOKEN_FLAG_COALESCE 2
#DEFINE TOKEN_FLAG_STRING   4
#DEFINE TOKEN_FLAG_CHARLIT  8
#DEFINE TOKEN_FLAG_UPPER    16

FN LexGetStringTokenInternal (
    IN token : ^LexToken,
    IN terminator : UBYTE,
    IN dynamicbuffer : ^TlDynamicBuffer,
    IN buffer : ^UBYTE,
    OUT length : ULONG,
)

    len := 0
    isbackslash := FALSE

    WHILE TRUE DO
        byte := LexGetCharacter ()

        IF NOT byte THEN
            LexError ( token, "String token terminated by EOF!\n" )
        END

        IF NOT isbackslash THEN
            IF byte == terminator THEN
                // String is done.

                BREAK

            ELSEIF byte == '\\' THEN
                isbackslash = TRUE

                CONTINUE
            END
        ELSE
            isbackslash = FALSE

            IF byte == 'n' THEN
                byte = '\n'
            
            ELSEIF byte == 't' THEN
                byte = '\t'
            
            ELSEIF byte == 'r' THEN
                byte = '\r'
            
            ELSEIF byte == 'b' THEN
                byte = '\b'
            
            ELSEIF byte == '[' THEN
                byte = '\['
            END
        END

        IF dynamicbuffer THEN
            TlInsertDynamicBuffer ( dynamicbuffer, byte )
        
        ELSEIF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexError ( token, "Token length exceeds maximum.\n" )
        
        ELSE
            buffer[len] = byte
            len += 1
        END
    END

    IF dynamicbuffer THEN
        TlInsertDynamicBuffer ( dynamicbuffer, 0 )
    ELSE
        buffer[len] = 0
        length = len
    END
END

FN LexCrunchCharacterLiteral (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : UWORD

    num : UWORD = 0
    i := 0

    WHILE buffer[i] DO
        lastnum := num

        num *= 256

        IF lastnum != 0 AND num / lastnum != 256 THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        num += buffer[i]

        IF num < lastnum THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        i += 1
    END

    RETURN num
END

FN LexCrunchNumber (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : UWORD

    num : UWORD = 0
    i := 0

    IF buffer[0] == '0' THEN
        // This number is either a literal zero, octal, or hexadecimal.

        IF NOT buffer[1] THEN
            // Literal zero.

            RETURN 0
        END

        IF buffer[1] == 'x' THEN
            // Hexadecimal.

            IF NOT buffer[2] THEN
                LexError ( token, "Unfinished hex number.\n" )
            END

            i = 2

            WHILE buffer[i] DO
                lastnum := num

                num *= 16

                // Check for overflow.

                IF lastnum != 0 AND num / lastnum != 16 THEN
                    LexError ( token, "Number does not fit in machine word.\n" )
                END

                IF buffer[i] >= '0' AND buffer[i] <= '9' THEN
                    num += buffer[i] - '0'
                
                ELSEIF buffer[i] >= 'A' AND buffer[i] <= 'F' THEN
                    num += buffer[i] - 'A' + 10
                
                ELSEIF buffer[i] >= 'a' AND buffer[i] <= 'f' THEN
                    num += buffer[i] - 'a' + 10

                ELSE
                    LexError ( token, "Malformed hex number.\n" )
                END

                // Check for overflow.

                IF num < lastnum THEN
                    LexError ( token, "Number does not fit in machine word.\n" )
                END

                i += 1
            END

            IF (num & JklTargetInfo^.ConstantMask) != num THEN
                LexError ( token, "Number does not fit in machine word.\n" )
            END

            RETURN num
        END

        // Octal.

        i = 1

        WHILE buffer[i] DO
            IF NOT (buffer[i] >= '0' AND buffer[i] <= '7') THEN
                LexError ( token, "Malformed octal number.\n" )
            END

            lastnum := num

            num *= 8

            // Check for overflow.

            IF lastnum != 0 AND num / lastnum != 8 THEN
                LexError ( token, "Number does not fit in machine word.\n" )
            END

            num += buffer[i] - '0'

            // Check for overflow.

            IF num < lastnum THEN
                LexError ( token, "Number does not fit in machine word.\n" )
            END

            i += 1
        END

        IF (num & JklTargetInfo^.ConstantMask) != num THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        RETURN num
    END

    // Decimal.

    WHILE buffer[i] DO
        IF NOT (buffer[i] >= '0' AND buffer[i] <= '9') THEN
            LexError ( token, "Malformed decimal number.\n" )
        END

        lastnum := num

        num *= 10

        // Check for overflow.

        IF lastnum != 0 AND num / lastnum != 10 THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        num += buffer[i] - '0'

        // Check for overflow.

        IF num < lastnum THEN
            LexError ( token, "Number does not fit in machine word.\n" )
        END

        i += 1
    END

    IF (num & JklTargetInfo^.ConstantMask) != num THEN
        LexError ( token, "Number does not fit in machine word.\n" )
    END

    RETURN num
END

FN LexGetTokenContents (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
    OUT length : ULONG,
    OUT internedstring : ^LexInternedString,
) : UBYTE

    // Reads the string of the next token into the provided buffer. Identifies
    // macros and expands them. Returns a set of flags indicating things about
    // the token; for instance, if EOF has been reached (i.e. there is no
    // token), if the token is all uppercase, and if the token is a SPLIT or
    // COALESCE token. The buffer is assumed to be LEX_TOKEN_STRING_SIZE bytes
    // in length or greater.

    // We GOTO LoopOnMacro in the case that the token turns out to be a macro
    // that needs expansion (this is intentionally picked over the alternative
    // of wrapping this routine in a WHILE TRUE loop).

    @LoopOnMacro

    length = 0

    // Skip leading whitespace.

    stream := LexCurrentStream

    IF stream THEN
        token^.FileBlock = stream^.FileBlock
        token^.Macro = stream^.Macro
        token^.LineNumber = stream^.LineNumber
        token^.LinePosition = stream^.LinePosition
    END

    byte := LexGetCharacter ()

    WHILE LexCharTreatment[byte] == CHAR_WHITESPACE DO
        // Re-capture the stream location information for each character of
        // whitespace consumed, until we get the real thing.

        stream = LexCurrentStream

        IF stream THEN
            token^.FileBlock = stream^.FileBlock
            token^.Macro = stream^.Macro
            token^.LineNumber = stream^.LineNumber
            token^.LinePosition = stream^.LinePosition
        END

        byte = LexGetCharacter ()
    END

    IF NOT byte THEN
        // EOF encountered.

        RETURN 0
    END

    IF byte == '\"' THEN
        // This is a string token. Allocate a LexInternedString.

        internedstring = CAST TlAllocateFromZone (
            &LexInternedStringZone, // zone
        ) TO ^LexInternedString

        internedstring^.CodeGenContext = 0

        IF LexCurrentSection == &LexDefaultSection THEN
            internedstring^.Section = &LexTextSection
        
        ELSE
            internedstring^.Section = LexCurrentSection
        END

        dynamicbuffer := &internedstring^.DynamicBuffer

        TlInitializeDynamicBuffer ( dynamicbuffer )

        dumpster : ULONG

        LexGetStringTokenInternal (
            token, // token
            '\"', // terminator
            dynamicbuffer, // dynamicbuffer
            NULLPTR, // buffer
            OUT dumpster, // length
        )

        // Lookup or insert the string in the interned string hash table.

        str := CAST TlLookupOrInsertHashTable (
            &LexCurrentSection^.InternedStringHashTable, // hashtable
            CAST internedstring TO ^TlHashTableEntry, // entry
            dynamicbuffer^.Buffer, // key
        ) TO ^LexInternedString

        IF str != internedstring THEN
            // Release our dynamic buffer and return the one we found.

            TlUninitializeDynamicBuffer ( dynamicbuffer )

            TlFreeToZone (
                &LexInternedStringZone, // zone
                internedstring, // block
            )

            internedstring = str
        END

        RETURN TOKEN_FLAG_STRING
    END

    IF byte == 0x27 THEN // single-quote
        // This is a character literal.

        LexGetStringTokenInternal (
            token, // token
            0x27, // terminator
            NULLPTR, // dynamicbuffer
            buffer, // buffer
            OUT length, // length
        )

        RETURN TOKEN_FLAG_CHARLIT
    END

    IF LexCharTreatment[byte] == CHAR_SPLIT THEN
        // This is a split token. It consists of exactly this character and no
        // more.

        length = 1
        buffer[0] = byte
        buffer[1] = 0

        RETURN TOKEN_FLAG_SPLIT
    END

    IF LexCharTreatment[byte] == CHAR_COALESCE THEN
        // This is a coalesce token. It consists of this character and the
        // following coalesce characters, and no more.

        len := 0

        WHILE LexCharTreatment[byte] == CHAR_COALESCE DO
            IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
                LexError ( token, "Token length exceeds maximum.\n" )
            END

            buffer[len] = byte
            len += 1

            byte = LexGetCharacter ()
        END

        buffer[len] = 0
        length = len

        IF LexCharTreatment[byte] != CHAR_WHITESPACE THEN
            // Set the putback character to the last one we saw, since that is part
            // of the next token.

            LexCurrentStream^.CoalescePutback = byte
        END

        RETURN TOKEN_FLAG_COALESCE
    END

    // This is a normal token. All we have to track is whether it consists only
    // of uppercase letters A-Z, in which case it must be a keyword. It could
    // also turn out to be a macro, in which case we must add it to the stream
    // stack and loop here.

    uppercaseonly := byte >= 'A' AND byte <= 'Z'

    len := 0

    WHILE LexCharTreatment[byte] == CHAR_NORMAL DO
        IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexError ( token, "Token length exceeds maximum.\n" )
        END

        IF uppercaseonly THEN
            uppercaseonly = byte >= 'A' AND byte <= 'Z'
        END

        buffer[len] = byte
        len += 1

        byte = LexGetCharacter ()
    END

    buffer[len] = 0
    length = len

    IF LexCharTreatment[byte] == CHAR_SPLIT OR
        LexCharTreatment[byte] == CHAR_COALESCE THEN

        LexCurrentStream^.CoalescePutback = byte
    END

    IF uppercaseonly AND len > 1 THEN
        RETURN TOKEN_FLAG_UPPER
    END

    // Check if this is a defined macro name. If so, place that macro on the top
    // of the stream stack and loop to the beginning of this procedure.
    //
    // N.B. Jackal macros are "hygienic", meaning that they have no access to
    // the "parent scope", except via their arguments, which become special
    // macros that restore the parent scope during their expansion. This
    // requires associating scopes with streams, so that we know what to restore
    // when we pop a stream off the stream stack.

    macro := CAST TlLookupSymbolTable (
        LexCurrentMacroScope, // symboltable
        &buffer[0], // name
    ) TO ^LexMacro

    IF NOT macro THEN
        // Not a defined macro, just return the token.

        RETURN 0
    END

    // It is a macro! Expand it and loop, so our caller gets the next "real"
    // token.

    LexExpandMacro (
        macro, // macro
        token, // token
    )

    GOTO LoopOnMacro
END

FN LexNextToken (
    IN token : ^LexToken,
)

    // Upper level part of token getting.
    // Receives a token string and some flags, and does some extra crunching on
    // it. Ignores the putback stack.

    internedstring : ^LexInternedString

    buffer : UBYTE[LEX_TOKEN_STRING_SIZE]
    length : ULONG

    tokenflag := LexGetTokenContents (
        token, // token
        &buffer[0], // buffer
        OUT length, // length
        OUT internedstring, // internedstring
    )

    IF tokenflag & ( TOKEN_FLAG_SPLIT | TOKEN_FLAG_COALESCE |
       TOKEN_FLAG_UPPER ) THEN

        // This must be a keyword token. If it doesn't hash to a keyword,
        // there's a syntax error.

        keyword := CAST TlLookupHashTable (
            &LexKeywordHashTable, // hashtable
            &buffer[0], // key
        ) TO ^LexKeyword

        IF NOT keyword THEN
            IF tokenflag & TOKEN_FLAG_UPPER THEN
                LexError ( token,
                "Bad use of uppercase alphabetic token (must be keyword).\n" )
            END

            LexError ( token, "Unknown keyword\n" )
        END

        token^.Type = keyword^.Type
        token^.Subtype = keyword^.Subtype
        token^.TypeContext = keyword^.TypeContext

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_CHARLIT THEN
        // This is a character literal token. We turn those into numerical
        // tokens.

        token^.Type = TOKEN_NUMBER
        token^.Payload = CAST LexCrunchCharacterLiteral (
            token, // token
            &buffer[0], // buffer
        ) TO ^VOID

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_STRING THEN
        // This is a string token. Record the interned string.

        token^.Type = TOKEN_STRING
        token^.Payload = internedstring

        LEAVE
    END

    IF NOT length THEN
        // EOF has been reached.

        token^.Type = TOKEN_EOF

        LEAVE
    END

    IF buffer[0] >= '0' AND buffer[0] <= '9' THEN
        // This is a numerical token.

        token^.Type = TOKEN_NUMBER
        token^.Payload = CAST LexCrunchNumber (
            token, // token
            &buffer[0], // buffer
        ) TO ^VOID

        LEAVE
    END

    // This is an identifier. We have to create an entry in the symbol table
    // for the current scope, if one does not exist.

    token^.Type = TOKEN_IDENTIFIER
    token^.Subtype = TOKEN_IDENTIFIER_FOUND

    symbol : ^LexSymbol

    IF LexCurrentOverlayScope THEN
        symbol = CAST TlLookupSymbolTable (
            LexCurrentOverlayScope, // symboltable
            &buffer[0], // name
        ) TO ^LexSymbol

    ELSE
        symbol = CAST TlLookupSymbolTable (
            LexCurrentScope, // symboltable
            &buffer[0], // name
        ) TO ^LexSymbol
    END

    IF NOT symbol THEN
        // Create new symbol from bump allocator.

        status := TlBumpAlloc (
            SIZEOF LexSymbol + length + 1, // bytes
            OUT symbol, // ptr
        )

        IF status THEN
            TlInternalError ( "Couldn't allocate symbol", 0, 0, 0 )
        END

        symbol^.Name = CAST (symbol + SIZEOF LexSymbol) TO ^UBYTE
        symbol^.Type = SYM_UNINITIALIZED
        symbol^.IrContext = NULLPTR
        symbol^.TrgContext = NULLPTR
        symbol^.StackAllocated = FALSE
        symbol^.ParameterType = ARG_NULL

        TlCopyMemory (
            symbol^.Name, // dest
            &buffer[0], // src
            length + 1, // sz (plus one for null terminator)
        )

        IF LexCurrentOverlayScope THEN
            TlInsertSymbolTable (
                LexCurrentOverlayScope, // symboltable
                CAST symbol TO ^TlHashTableEntry, // entry
                symbol^.Name, // name
            )

        ELSE
            TlInsertSymbolTable (
                LexCurrentScope, // symboltable
                CAST symbol TO ^TlHashTableEntry, // entry
                symbol^.Name, // name
            )
        END

        token^.Subtype = TOKEN_IDENTIFIER_NEW
    END

    token^.Payload = symbol
END

FN LexEnterScope (
    IN scope : ^TlSymbolTable,
) : ^TlSymbolTable

    // Enter a new scope.

    oldscope := LexCurrentScope

    IF NOT scope THEN
        LexCurrentScope = TlCreateSymbolTable (
            oldscope, // outerscope
            &LexDeleteSymbol, // deletefunc
        )
    ELSE
        LexCurrentScope = scope
    END

    RETURN oldscope
END

FN LexResetScope (
    IN scope : ^TlSymbolTable,
) : ^TlSymbolTable

    oldscope := LexCurrentScope

    LexCurrentScope = scope

    RETURN oldscope
END

FN LexLeaveScope ()

    // Leave the current scope.

    LexCurrentScope = TlDeleteSymbolTable ( LexCurrentScope )
END

FN LexEnterOverlayScope (
    IN scope : ^TlSymbolTable,
)

    // Overlay the scope.

    LexCurrentOverlayScope = scope
END

FN LexExitOverlayScope ()

    // Un-overlay the scope.

    LexCurrentOverlayScope = NULLPTR
END

FN LexCopyToken (
    IN dest : ^LexToken,
    IN src : ^LexToken,
)

    // Copy all the fields of a source token into a destination token.

    dest^.Payload = src^.Payload
    dest^.FileBlock = src^.FileBlock
    dest^.LineNumber = src^.LineNumber
    dest^.LinePosition = src^.LinePosition
    dest^.Type = src^.Type
    dest^.Subtype = src^.Subtype
    dest^.TypeContext = src^.TypeContext
    dest^.Macro = src^.Macro
END

FN LexGetToken (
    IN token : ^LexToken,
)

    // Read a token from the putback stack if one is present, otherwise read
    // from the source stream.

    index := LexPutbackStackPtr

    IF index THEN
        LexCopyToken (
            token, // dest
            &LexPutbackStack[index - 1], // src
        )

        LexPutbackStackPtr = index - 1

        LEAVE
    END

    LexNextToken ( token )
END

FN LexPutbackToken (
    IN token : ^LexToken,
)

    // Put the token on top of the putback stack.

    index := LexPutbackStackPtr

    IF index == LEX_PUTBACK_STACK_DEPTH THEN
        TlInternalError ( "Lexer putback stack overflow", 0, 0, 0 )
    END

    LexCopyToken (
        &LexPutbackStack[index], // dest
        token, // src
    )

    LexPutbackStackPtr = index + 1
END

FN LexMatchToken (
    IN token : ^LexToken,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
) : UBYTE

    // Read the next token into the caller's token structure, but only if it
    // matches the type and subtype. If the type or subtype are specified as
    // TOKEN_ANY or TOKEN_SUBTYPE_ANY respectively, then it will match any type
    // or subtype of token. Returns TRUE if a token was read, FALSE otherwise.

    index := LexPutbackStackPtr

    IF NOT index THEN
        // The putback stack is empty, read one token directly into the top.

        LexNextToken (
            &LexPutbackStack[0], // token
        )

        LexPutbackStackPtr = 1
        index = 1
    END

    matchtoken := &LexPutbackStack[index - 1]

    IF token THEN
        LexCopyToken (
            token, // dest
            matchtoken, // src
        )
    END

    IF matchtoken^.Type != type THEN
        // Not a match.

        IF matchtoken^.Type == TOKEN_EOF THEN
            // Don't save EOF tokens.

            LexPutbackStackPtr -= 1
        END

        RETURN FALSE
    END

    IF NOT subtype OR matchtoken^.Subtype == subtype THEN
        // Token matches! Pop it.

        LexPutbackStackPtr = index - 1

        RETURN TRUE
    END

    RETURN FALSE
END